# -*- coding: utf-8 -*-
"""International education affordability - GBoost classifier

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VerFOpxEGtbwNUD30VR1AfZUJwnJPhCe
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, RocCurveDisplay


sns.set(style="whitegrid")
pd.set_option('display.max_columns', None)

try:
    df = pd.read_csv('cost.csv')
    print(" Dataset loaded successfully.")
except FileNotFoundError:
    print(" Error: Dataset not found. Please upload 'dataset.csv'.")


print("\n First 5 rows of the dataset:")
display(df.head())

print("\nℹ Dataset Info:")
df.info()

print("\n Dataset Description:")
display(df.describe())

df['Living_Expense_Yearly_USD'] = (df['Rent_USD'] * 12) * (df['Living_Cost_Index'] /100)

df['Total_Cost_USD'] = (
    df['Tuition_USD']
  + df['Rent_USD'] * 12 * df['Duration_Years']
  + df['Living_Expense_Yearly_USD'] * df['Duration_Years']
  + df['Insurance_USD'] * df['Duration_Years']
  + df['Visa_Fee_USD']
)



threshold = df['Total_Cost_USD'].median()
df['Cost_Label'] = np.where(df['Total_Cost_USD'] <= threshold, 'Affordable', 'Expensive')
print("median cost in dollars: $",threshold,"")

print("\n Class Distribution (%):")
print(df['Cost_Label'].value_counts(normalize=True) * 100)

sns.countplot(x='Cost_Label', data=df)
plt.title('Affordable vs Expensive Distribution')
plt.ylabel('Count')
plt.xlabel('Category')
plt.show()

print("\n Missing Values Count:")
print(df.isnull().sum())

print("\n Missing Values (%):")
missing_percent = df.isnull().sum() * 100 / len(df)
print(missing_percent)

# Heatmap of missing values (if any)
if df.isnull().sum().sum() > 0:
    plt.figure(figsize=(12,6))
    sns.heatmap(df.isnull(), cbar=False, cmap='viridis')
    plt.title("Heatmap of Missing Values")
    plt.show()

# Correlation heatmap of numeric features
plt.figure(figsize=(12,8))
sns.heatmap(df.corr(numeric_only=True), annot=True, fmt=".2f", cmap="coolwarm", linewidths=0.5)
plt.title("Correlation Heatmap of Numerical Features")
plt.show()

numerics = df.select_dtypes(include=['float64', 'int64']).columns
df[numerics] = df[numerics].fillna(df[numerics].mean())
# Ensure Cost_Label exists
if 'Cost_Label' not in df.columns:
    threshold = df['Total_Cost_USD'].median()
    df['Cost_Label'] = np.where(df['Total_Cost_USD'] <= threshold, 'Affordable', 'Expensive')

# Encode categorical features
cat_cols = ['Country', 'City', 'University', 'Program', 'Level']
label_encoders = {}
for col in cat_cols:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

# Prepare features and target
features = [
    'Country', 'City', 'University', 'Program', 'Level',
    'Duration_Years', 'Tuition_USD', 'Rent_USD', 'Visa_Fee_USD',
    'Insurance_USD', 'Living_Cost_Index', 'Exchange_Rate'
]
X = df[features]
y = df['Cost_Label'].map({'Affordable': 0, 'Expensive': 1})

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

print(f"\n✅ Training Set Shape: {X_train.shape}, Testing Set Shape: {X_test.shape}")

if 'Cost_Label' not in df.columns:
    threshold = df['Total_Cost_USD'].median()
    df['Cost_Label'] = np.where(df['Total_Cost_USD'] <= threshold, 'Affordable', 'Expensive')

# Fill missing numeric values with column mean
numerics = df.select_dtypes(include=['float64', 'int64']).columns
df[numerics] = df[numerics].fillna(df[numerics].mean())

# Encode categorical features
cat_cols = ['Country', 'City', 'University', 'Program', 'Level']
label_encoders = {}
for col in cat_cols:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

# Prepare features and target
features = [
    'Country', 'City', 'University', 'Program', 'Level',
    'Duration_Years', 'Tuition_USD', 'Rent_USD', 'Visa_Fee_USD',
    'Insurance_USD', 'Living_Cost_Index', 'Exchange_Rate'
]
X = df[features]
print(X)
y = df['Cost_Label'].map({'Affordable': 0, 'Expensive': 1})

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

print(f"\n Training Set Shape: {X_train.shape}, Testing Set Shape: {X_test.shape}")

# Bonus Insight: Boxplot of Total Cost by Cost_Label
plt.figure(figsize=(8,6))
sns.boxplot(x=df['Cost_Label'], y=df['Total_Cost_USD'], palette='Set2')
plt.title('Distribution of Total Cost by Affordability Category')
plt.ylabel('Total Cost (USD)')
plt.show()

# Insight:
print("\n Observation:")
print("As expected, the 'Expensive' category has a significantly higher total cost distribution than the 'Affordable' one, validating the correctness of our target label based on the median cost.")

#  Feature Importance - SHAP and PDP
import shap
from sklearn.inspection import PartialDependenceDisplay

# Initialize SHAP explainer and compute values
explainer = shap.Explainer(gb_best)
shap_values = explainer(X_train)

# SHAP summary plot
shap.summary_plot(shap_values, X_train, plot_type="bar")

# Identify top 3 important features from model directly
top_features = pd.Series(gb_best.feature_importances_, index=X_train.columns).sort_values(ascending=False).head(3).index.tolist()

# Partial Dependence Plots
print(f"\n PDP for Top Features: {top_features}")
fig, ax = plt.subplots(1, len(top_features), figsize=(5*len(top_features), 4))
for i, feature in enumerate(top_features):
    PartialDependenceDisplay.from_estimator(gb_best, X_train, [feature], ax=ax[i])
plt.tight_layout()
plt.show()

# : Modeling with Multiple Classifiers and Comparing Accuracy
models = {
    "Logistic Regression": LogisticRegression(solver='liblinear'),
    "K-Nearest Neighbors": KNeighborsClassifier(),
    "Random Forest": RandomForestClassifier(),
    "Gradient Boosting": GradientBoostingClassifier()
}

def fit_and_score(models, X_train, X_test, y_train, y_test):
    model_scores = {}
    for name, model in models.items():
        model.fit(X_train, y_train)
        score = model.score(X_test, y_test)
        model_scores[name] = score
        print(f" {name} Accuracy: {score:.4f}")
    return model_scores

model_scores = fit_and_score(models, X_train, X_test, y_train, y_test)

# Bar chart for comparison
score_df = pd.DataFrame(model_scores, index=['Accuracy']).T.sort_values(by='Accuracy', ascending=True)
plt.figure(figsize=(10,6))
sns.barplot(x=score_df.Accuracy, y=score_df.index, palette="crest")
plt.title("Model Accuracy Comparison")
plt.xlabel("Accuracy")
plt.ylabel("Classifier")
plt.xlim(0.5, 1)
plt.show()

from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
import numpy as np

#  : Hyperparameter Tuning - Gradient Boosting
gb_grid = {
    "n_estimators": np.arange(100, 500, 100),
    "learning_rate": [0.01, 0.05, 0.1, 0.2],
    "max_depth": [3, 5, 7],
    "min_samples_split": [2, 5, 10],
    "min_samples_leaf": [1, 2, 4]
}

rs_gb = RandomizedSearchCV(GradientBoostingClassifier(),
                           param_distributions=gb_grid,
                           n_iter=20,
                           cv=5,
                           verbose=1,
                           random_state=42,
                           n_jobs=-1)

rs_gb.fit(X_train, y_train)
print("\n Best Parameters (Gradient Boosting - RandomizedSearchCV):")
print(rs_gb.best_params_)

# GridSearchCV refinement
gs_gb_grid = {
    "n_estimators": [rs_gb.best_params_["n_estimators"]],
    "learning_rate": [rs_gb.best_params_["learning_rate"]],
    "max_depth": [rs_gb.best_params_["max_depth"]],
    "min_samples_split": [rs_gb.best_params_["min_samples_split"]],
    "min_samples_leaf": [rs_gb.best_params_["min_samples_leaf"]]
}

gs_gb = GridSearchCV(GradientBoostingClassifier(),
                     param_grid=gs_gb_grid,
                     cv=5,
                     verbose=1,
                     n_jobs=-1)

gs_gb.fit(X_train, y_train)
print("\n Best Parameters (Gradient Boosting - GridSearchCV):")
print(gs_gb.best_params_)

# Evaluate tuned model
gb_best = gs_gb.best_estimator_
y_preds_gb = gb_best.predict(X_test)
gb_acc = accuracy_score(y_test, y_preds_gb)
print(f"\n Tuned Gradient Boosting Accuracy: {gb_acc:.4f}")


ConfusionMatrixDisplay.from_estimator(
    gb_best,
    X_test,
    y_test,
    display_labels=["Affordable", "Expensive"],
    cmap="Blues"
)
plt.title("Confusion Matrix - GBoost")
plt.show()

# Final Classification of All Universities

df['Predicted_Label'] = gb_best.predict(X)
df['Affordability'] = df['Predicted_Label'].map({0: 'Affordable', 1: 'Expensive'})

# Decode encoded university IDs back to names if needed
df['University_Name'] = label_encoders['University'].inverse_transform(df['University'])

# Get affordable and expensive university names
affordable_unis = sorted(df[df['Affordability'] == 'Affordable']['University_Name'].unique())
expensive_unis = sorted(df[df['Affordability'] == 'Expensive']['University_Name'].unique())

print("\n Affordable Universities:")
print(", ".join(map(str, affordable_unis)))

print("\n Expensive Universities:")
print(", ".join(map(str, expensive_unis)))

# Summary insight
print("\n Insight:")
print(f"Out of {df['University_Name'].nunique()} unique universities, {len(affordable_unis)} are predicted as Affordable and {len(expensive_unis)} as Expensive by the final model.")

#  : Hyperparameter Tuning - Gradient Boosting
gb_grid = {
    "n_estimators": np.arange(100, 500, 100),
    "learning_rate": [0.01, 0.05, 0.1, 0.2],
    "max_depth": [3, 5, 7],
    "min_samples_split": [2, 5, 10],
    "min_samples_leaf": [1, 2, 4]
}

rs_gb = RandomizedSearchCV(GradientBoostingClassifier(),
                           param_distributions=gb_grid,
                           n_iter=20,
                           cv=5,
                           verbose=1,
                           random_state=42,
                           n_jobs=-1)

rs_gb.fit(X_train, y_train)
print("\n Best Parameters (Gradient Boosting - RandomizedSearchCV):")
print(rs_gb.best_params_)

# GridSearchCV refinement
gs_gb_grid = {
    "n_estimators": [rs_gb.best_params_["n_estimators"]],
    "learning_rate": [rs_gb.best_params_["learning_rate"]],
    "max_depth": [rs_gb.best_params_["max_depth"]],
    "min_samples_split": [rs_gb.best_params_["min_samples_split"]],
    "min_samples_leaf": [rs_gb.best_params_["min_samples_leaf"]]
}

gs_gb = GridSearchCV(GradientBoostingClassifier(),
                     param_grid=gs_gb_grid,
                     cv=5,
                     verbose=1,
                     n_jobs=-1)

gs_gb.fit(X_train, y_train)
print("\n Best Parameters (Gradient Boosting - GridSearchCV):")
print(gs_gb.best_params_)

# Evaluate tuned model
gb_best = gs_gb.best_estimator_
y_preds_gb = gb_best.predict(X_test)
gb_acc = accuracy_score(y_test, y_preds_gb)
print(f"\n Tuned Gradient Boosting Accuracy: {gb_acc:.4f}")

